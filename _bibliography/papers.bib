---
---
@inproceedings{yeo2022engorgio,
  selected={true},
  title={Engorgio: Neural Video Enhancement at Scale},
  author={Yeo, Hyunho and Lim, Hwijoon and Kim, Jaehong and Jung, Youngmok and Ye, Juncheol and Han, Dongsu},
  booktitle={Proceedings of the ACM SIGCOMM 2022 conference on SIGCOMM (Will appear)},
  year={2022},
  month={Aug},
  abbr={SIGCOMM}
}

@inproceedings{10.1145/3372224.3419185,
abbr={MobiCom},
selected={true},
website={https://bit.ly/37faRT9},
abstract={The demand for mobile video streaming has experienced tremendous growth over the last decade. However, existing methods of video delivery fall short of delivering high-quality video. Recent advances in neural super-resolution have opened up the possibility of enhancing video quality by leveraging client-side computation. Unfortunately, mobile devices cannot benefit from this because it is too expensive in computation and power-hungry.
To overcome the limitation, we present NEMO, a system that enables real-time video super-resolution on mobile devices. NEMO applies neural super-resolution to a few select frames and transfers the outputs to benefit the remaining frames. The frames to which super-resolution is applied are carefully chosen to maximize the overall quality gains. NEMO leverages fine-grained dependencies using information from the video codec and strives to provide guarantees in the quality degradation compared to per-frame super-resolution. Our evaluation using a full system implementation on Android shows NEMO improves the overall processing throughput by x11.5, reduces energy consumption by 88.6%, and maintains device temperatures at acceptable levels compared to per-frame super-resolution, while ensuring high video quality. Overall, this leads to a 31.2% improvement in quality of experience for mobile users.},
pdf={3372224.3419185.pdf},
author = {Yeo, Hyunho and Chong, Chan Ju and Jung, Youngmok and Ye, Juncheol and Han, Dongsu},
title = {NEMO: Enabling Neural-Enhanced Video Streaming on Commodity Mobile Devices},
year = {2020},
isbn = {9781450370851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372224.3419185},
abstract = {The demand for mobile video streaming has experienced tremendous growth over the last decade. However, existing methods of video delivery fall short of delivering high-quality video. Recent advances in neural super-resolution have opened up the possibility of enhancing video quality by leveraging client-side computation. Unfortunately, mobile devices cannot benefit from this because it is too expensive in computation and power-hungry.To overcome the limitation, we present NEMO, a system that enables real-time video super-resolution on mobile devices. NEMO applies neural super-resolution to a few select frames and transfers the outputs to benefit the remaining frames. The frames to which super-resolution is applied are carefully chosen to maximize the overall quality gains. NEMO leverages fine-grained dependencies using information from the video codec and strives to provide guarantees in the quality degradation compared to per-frame super-resolution. Our evaluation using a full system implementation on Android shows NEMO improves the overall processing throughput by x11.5, reduces energy consumption by 88.6%, and maintains device temperatures at acceptable levels compared to per-frame super-resolution, while ensuring high video quality. Overall, this leads to a 31.2% improvement in quality of experience for mobile users.},
booktitle = {Proceedings of the 26th Annual International Conference on Mobile Computing and Networking},
articleno = {28},
numpages = {14}
}


@inproceedings{10.1145/3387514.3405856,
abbr={SIGCOMM},
selected={true},
abstract={Live video accounts for a significant volume of today's Internet video. Despite a large number of efforts to enhance user quality of experience (QoE) both at the ingest and distribution side of live video, the fundamental limitations are that streamer's upstream bandwidth and computational capacity limit the quality of experience of thousands of viewers.
To overcome this limitation, we design LiveNAS, a new live video ingest framework that enhances the origin stream's quality by leveraging computation at ingest servers. Our ingest server applies neural super-resolution on the original stream, while imposing minimal overhead on ingest clients. LiveNAS employs online learning to maximize the quality gain and dynamically adjusts the resource use to the real-time quality improvement. LiveNAS delivers high-quality live streams up to 4K resolution, outperforming WebRTC by 1.96 dB on average in Peak-Signal-to-Noise-Ratio on real video streams and network traces, which leads to 12%-69% QoE improvement for live stream viewers.},
pdf={3387514.3405856.pdf},
website={https://bit.ly/3KxbCW5},
author = {Kim^, Jaehong and Jung^, Youngmok and Yeo, Hyunho and Ye, Juncheol and Han, Dongsu},
title = {Neural-Enhanced Live Streaming: Improving Live Video Ingest via Online Learning},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405856},
doi = {10.1145/3387514.3405856},
abstract = {Live video accounts for a significant volume of today's Internet video. Despite a large number of efforts to enhance user quality of experience (QoE) both at the ingest and distribution side of live video, the fundamental limitations are that streamer's upstream bandwidth and computational capacity limit the quality of experience of thousands of viewers.To overcome this limitation, we design LiveNAS, a new live video ingest framework that enhances the origin stream's quality by leveraging computation at ingest servers. Our ingest server applies neural super-resolution on the original stream, while imposing minimal overhead on ingest clients. LiveNAS employs online learning to maximize the quality gain and dynamically adjusts the resource use to the real-time quality improvement. LiveNAS delivers high-quality live streams up to 4K resolution, outperforming WebRTC by 1.96 dB on average in Peak-Signal-to-Noise-Ratio on real video streams and network traces, which leads to 12%-69% QoE improvement for live stream viewers.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {107–125},
numpages = {19},
keywords = {live streaming, online learning, super-resolution, video delivery, deep neural networks},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}


@inproceedings {222555,
abbr={OSDI},
selected={true},
abstract={Internet video streaming has experienced tremendous growth over the last few decades. However, the quality of existing video delivery critically depends on the bandwidth resource. Consequently, user quality of experience (QoE) suffers inevitably when network conditions become unfavorable. We present a new video delivery framework that utilizes client computation and recent advances in deep neural networks (DNNs) to reduce the dependency for delivering high-quality video. The use of DNNs enables us to enhance the video quality independent to the available bandwidth. We design a practical system that addresses several challenges, such as client heterogeneity, interaction with bitrate adaptation, and DNN transfer, in enabling the idea. Our evaluation using 3G and broadband network traces shows the proposed system outperforms the current state of the art, enhancing the average QoE by 43.08% using the same bandwidth budget or saving 17.13% of bandwidth while providing the same user QoE.},
pdf={osdi18yeo.pdf},
website={https://bit.ly/3IWStw7},
author = {Hyunho Yeo and Youngmok Jung and Jaehong Kim and Jinwoo Shin and Dongsu Han},
title = {Neural Adaptive Content-aware Internet Video Delivery},
booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
year = {2018},
isbn = {978-1-939133-08-3},
address = {Carlsbad, CA},
pages = {645--661},
url = {https://www.usenix.org/conference/osdi18/presentation/yeo},
publisher = {USENIX Association},
month = oct,
}

@inproceedings{10.1145/3152434.3152440,
abbr={HotNets},
selected={false},
abstract={Much effort has been put into video delivery infrastructure for scaling video delivery and enhancing quality of experience.
Existing designs successfully combine traditional systems and networking approaches to achieve the goals.
This paper explores unforeseen opportunities  enabled by recent advances in deep learning.
Traditional video delivery relies on standard signal processing, while deep neural networks (DNNs) present unprecedented ways in handling images and video data.
In particular, leveraging deep neural networks opens up new possibilities in exploiting long-term redundancies found in video. Based on this observation, we show multiple case studies that showcase how content delivery infrastructure can utilize DNNs to effectively enhance the video quality at reduced bandwidth overhead.
We examine its implications on video delivery by exploring the design space for DNN-based content-aware video delivery.
Finally, we quantify the potential benefit and overhead of the design through  proof-of-concept evaluation on real video.},
pdf={3152434.3152440.pdf},
author = {Hyunho Yeo and Sunghyun Do and Dongsu Han},
title = {How Will Deep Learning Change Internet Video Delivery?},
year = {2017},
isbn = {9781450355698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3152434.3152440},
doi = {10.1145/3152434.3152440},
booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Networks},
pages = {57–64},
numpages = {8},
location = {Palo Alto, CA, USA},
series = {HotNets-XVI}
}

